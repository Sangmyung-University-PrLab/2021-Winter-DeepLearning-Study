{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# GPU 자원 사용확인\n",
    "devices_id = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #사용 가능하면 GPU 아니면 cpu\n",
    "torch.cuda.set_device(\n",
    "    devices_id\n",
    ")  # fix bug for `ERROR: all tensors must be on devices[0]`\n",
    "\n",
    "# Create Tensorboard SummaryWriter instance\n",
    "writer_deep = SummaryWriter('./summary/deep_without_activation')\n",
    "writer_shallow = SummaryWriter('./summary/shallow_without_activation')\n",
    "\n",
    "# Step 1. Load Dataset\n",
    "train_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=True, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "test_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=False, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "\n",
    "# Step 2. Make Dataset Iterable\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create Model Class\n",
    "class Deep_LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Deep_LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 300)\n",
    "        self.linear2 = torch.nn.Linear(300, int(input_dim / 4))  # 392x196\n",
    "        self.linear3 = torch.nn.Linear(int(input_dim / 4), output_dim)  # 196x10\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear1(x)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.linear3(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Step 3. Create Model Class\n",
    "class Shallow_LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Shallow_LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear1(x)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "# [test] 만일 MSE을 LOSS 함수로 쓴다면???\n",
    "# output_dim = 1\n",
    "lr_rate = 0.01\n",
    "\n",
    "# Step 4. Instantiate Model Class\n",
    "model_deep = Deep_LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model_deep = nn.DataParallel(model_deep, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model_deep = nn.DataParallel(model_deep, device_ids=[devices_id]).cuda()\n",
    "\n",
    "model_shallow = Shallow_LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model_shallow = nn.DataParallel(model_shallow, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model_shallow = nn.DataParallel(model_shallow, device_ids=[devices_id]).cuda()\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss()  # computes softmax and then the cross entropy\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer_deep = torch.optim.SGD(model_deep.parameters(), lr=lr_rate)\n",
    "optimizer_shallow = torch.optim.SGD(model_shallow.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Deep] [Epoch 0] [Iteration: 199/600] [Loss: 1.800] [Accuracy: 68.60]\n",
      "[Shal] [Epoch 0] [Iteration: 199/600] [Loss: 1.162] [Accuracy: 81.12]\n",
      "[Deep] [Epoch 0] [Iteration: 399/600] [Loss: 1.056] [Accuracy: 74.94]\n",
      "[Shal] [Epoch 0] [Iteration: 399/600] [Loss: 0.821] [Accuracy: 83.93]\n",
      "[Deep] [Epoch 0] [Iteration: 599/600] [Loss: 0.725] [Accuracy: 81.79]\n",
      "[Shal] [Epoch 0] [Iteration: 599/600] [Loss: 0.681] [Accuracy: 85.34]\n",
      "[Deep] [Epoch 1] [Iteration: 199/600] [Loss: 0.609] [Accuracy: 85.10]\n",
      "[Shal] [Epoch 1] [Iteration: 199/600] [Loss: 0.674] [Accuracy: 86.51]\n",
      "[Deep] [Epoch 1] [Iteration: 399/600] [Loss: 0.489] [Accuracy: 86.68]\n",
      "[Shal] [Epoch 1] [Iteration: 399/600] [Loss: 0.582] [Accuracy: 86.86]\n",
      "[Deep] [Epoch 1] [Iteration: 599/600] [Loss: 0.490] [Accuracy: 87.68]\n",
      "[Shal] [Epoch 1] [Iteration: 599/600] [Loss: 0.563] [Accuracy: 87.35]\n",
      "[Deep] [Epoch 2] [Iteration: 199/600] [Loss: 0.369] [Accuracy: 88.41]\n",
      "[Shal] [Epoch 2] [Iteration: 199/600] [Loss: 0.522] [Accuracy: 87.69]\n",
      "[Deep] [Epoch 2] [Iteration: 399/600] [Loss: 0.334] [Accuracy: 89.02]\n",
      "[Shal] [Epoch 2] [Iteration: 399/600] [Loss: 0.445] [Accuracy: 88.08]\n",
      "[Deep] [Epoch 2] [Iteration: 599/600] [Loss: 0.472] [Accuracy: 89.24]\n",
      "[Shal] [Epoch 2] [Iteration: 599/600] [Loss: 0.565] [Accuracy: 88.21]\n",
      "[Deep] [Epoch 3] [Iteration: 199/600] [Loss: 0.535] [Accuracy: 89.67]\n",
      "[Shal] [Epoch 3] [Iteration: 199/600] [Loss: 0.581] [Accuracy: 88.43]\n",
      "[Deep] [Epoch 3] [Iteration: 399/600] [Loss: 0.334] [Accuracy: 89.86]\n",
      "[Shal] [Epoch 3] [Iteration: 399/600] [Loss: 0.445] [Accuracy: 88.74]\n",
      "[Deep] [Epoch 3] [Iteration: 599/600] [Loss: 0.462] [Accuracy: 89.90]\n",
      "[Shal] [Epoch 3] [Iteration: 599/600] [Loss: 0.561] [Accuracy: 88.73]\n",
      "[Deep] [Epoch 4] [Iteration: 199/600] [Loss: 0.432] [Accuracy: 90.07]\n",
      "[Shal] [Epoch 4] [Iteration: 199/600] [Loss: 0.529] [Accuracy: 89.02]\n",
      "[Deep] [Epoch 4] [Iteration: 399/600] [Loss: 0.350] [Accuracy: 90.34]\n",
      "[Shal] [Epoch 4] [Iteration: 399/600] [Loss: 0.460] [Accuracy: 89.13]\n",
      "[Deep] [Epoch 4] [Iteration: 599/600] [Loss: 0.242] [Accuracy: 90.40]\n",
      "[Shal] [Epoch 4] [Iteration: 599/600] [Loss: 0.343] [Accuracy: 89.28]\n",
      "[Deep] [Epoch 5] [Iteration: 199/600] [Loss: 0.288] [Accuracy: 90.75]\n",
      "[Shal] [Epoch 5] [Iteration: 199/600] [Loss: 0.389] [Accuracy: 89.41]\n",
      "[Deep] [Epoch 5] [Iteration: 399/600] [Loss: 0.321] [Accuracy: 90.80]\n",
      "[Shal] [Epoch 5] [Iteration: 399/600] [Loss: 0.458] [Accuracy: 89.51]\n",
      "[Deep] [Epoch 5] [Iteration: 599/600] [Loss: 0.217] [Accuracy: 90.80]\n",
      "[Shal] [Epoch 5] [Iteration: 599/600] [Loss: 0.307] [Accuracy: 89.65]\n",
      "[Deep] [Epoch 6] [Iteration: 199/600] [Loss: 0.324] [Accuracy: 90.98]\n",
      "[Shal] [Epoch 6] [Iteration: 199/600] [Loss: 0.384] [Accuracy: 89.77]\n",
      "[Deep] [Epoch 6] [Iteration: 399/600] [Loss: 0.397] [Accuracy: 91.16]\n",
      "[Shal] [Epoch 6] [Iteration: 399/600] [Loss: 0.470] [Accuracy: 89.81]\n",
      "[Deep] [Epoch 6] [Iteration: 599/600] [Loss: 0.296] [Accuracy: 91.11]\n",
      "[Shal] [Epoch 6] [Iteration: 599/600] [Loss: 0.377] [Accuracy: 89.77]\n",
      "[Deep] [Epoch 7] [Iteration: 199/600] [Loss: 0.405] [Accuracy: 91.13]\n",
      "[Shal] [Epoch 7] [Iteration: 199/600] [Loss: 0.479] [Accuracy: 89.86]\n",
      "[Deep] [Epoch 7] [Iteration: 399/600] [Loss: 0.456] [Accuracy: 91.21]\n",
      "[Shal] [Epoch 7] [Iteration: 399/600] [Loss: 0.510] [Accuracy: 89.85]\n",
      "[Deep] [Epoch 7] [Iteration: 599/600] [Loss: 0.214] [Accuracy: 91.44]\n",
      "[Shal] [Epoch 7] [Iteration: 599/600] [Loss: 0.302] [Accuracy: 90.05]\n",
      "[Deep] [Epoch 8] [Iteration: 199/600] [Loss: 0.369] [Accuracy: 91.44]\n",
      "[Shal] [Epoch 8] [Iteration: 199/600] [Loss: 0.435] [Accuracy: 90.08]\n",
      "[Deep] [Epoch 8] [Iteration: 399/600] [Loss: 0.263] [Accuracy: 91.35]\n",
      "[Shal] [Epoch 8] [Iteration: 399/600] [Loss: 0.369] [Accuracy: 90.11]\n",
      "[Deep] [Epoch 8] [Iteration: 599/600] [Loss: 0.293] [Accuracy: 91.35]\n",
      "[Shal] [Epoch 8] [Iteration: 599/600] [Loss: 0.395] [Accuracy: 90.18]\n",
      "[Deep] [Epoch 9] [Iteration: 199/600] [Loss: 0.350] [Accuracy: 91.52]\n",
      "[Shal] [Epoch 9] [Iteration: 199/600] [Loss: 0.455] [Accuracy: 90.24]\n",
      "[Deep] [Epoch 9] [Iteration: 399/600] [Loss: 0.231] [Accuracy: 91.71]\n",
      "[Shal] [Epoch 9] [Iteration: 399/600] [Loss: 0.327] [Accuracy: 90.25]\n",
      "[Deep] [Epoch 9] [Iteration: 599/600] [Loss: 0.238] [Accuracy: 91.65]\n",
      "[Shal] [Epoch 9] [Iteration: 599/600] [Loss: 0.372] [Accuracy: 90.34]\n",
      "[Deep] [Epoch 10] [Iteration: 199/600] [Loss: 0.391] [Accuracy: 91.73]\n",
      "[Shal] [Epoch 10] [Iteration: 199/600] [Loss: 0.462] [Accuracy: 90.36]\n",
      "[Deep] [Epoch 10] [Iteration: 399/600] [Loss: 0.213] [Accuracy: 91.70]\n",
      "[Shal] [Epoch 10] [Iteration: 399/600] [Loss: 0.303] [Accuracy: 90.42]\n",
      "[Deep] [Epoch 10] [Iteration: 599/600] [Loss: 0.205] [Accuracy: 91.69]\n",
      "[Shal] [Epoch 10] [Iteration: 599/600] [Loss: 0.265] [Accuracy: 90.42]\n",
      "[Deep] [Epoch 11] [Iteration: 199/600] [Loss: 0.174] [Accuracy: 91.76]\n",
      "[Shal] [Epoch 11] [Iteration: 199/600] [Loss: 0.267] [Accuracy: 90.48]\n",
      "[Deep] [Epoch 11] [Iteration: 399/600] [Loss: 0.247] [Accuracy: 91.76]\n",
      "[Shal] [Epoch 11] [Iteration: 399/600] [Loss: 0.359] [Accuracy: 90.52]\n",
      "[Deep] [Epoch 11] [Iteration: 599/600] [Loss: 0.297] [Accuracy: 91.83]\n",
      "[Shal] [Epoch 11] [Iteration: 599/600] [Loss: 0.346] [Accuracy: 90.57]\n",
      "[Deep] [Epoch 12] [Iteration: 199/600] [Loss: 0.285] [Accuracy: 91.84]\n",
      "[Shal] [Epoch 12] [Iteration: 199/600] [Loss: 0.349] [Accuracy: 90.63]\n",
      "[Deep] [Epoch 12] [Iteration: 399/600] [Loss: 0.221] [Accuracy: 91.79]\n",
      "[Shal] [Epoch 12] [Iteration: 399/600] [Loss: 0.327] [Accuracy: 90.62]\n",
      "[Deep] [Epoch 12] [Iteration: 599/600] [Loss: 0.596] [Accuracy: 91.86]\n",
      "[Shal] [Epoch 12] [Iteration: 599/600] [Loss: 0.595] [Accuracy: 90.65]\n",
      "[Deep] [Epoch 13] [Iteration: 199/600] [Loss: 0.304] [Accuracy: 91.88]\n",
      "[Shal] [Epoch 13] [Iteration: 199/600] [Loss: 0.389] [Accuracy: 90.71]\n",
      "[Deep] [Epoch 13] [Iteration: 399/600] [Loss: 0.326] [Accuracy: 91.98]\n",
      "[Shal] [Epoch 13] [Iteration: 399/600] [Loss: 0.399] [Accuracy: 90.75]\n",
      "[Deep] [Epoch 13] [Iteration: 599/600] [Loss: 0.255] [Accuracy: 91.90]\n",
      "[Shal] [Epoch 13] [Iteration: 599/600] [Loss: 0.382] [Accuracy: 90.70]\n",
      "[Deep] [Epoch 14] [Iteration: 199/600] [Loss: 0.249] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 14] [Iteration: 199/600] [Loss: 0.330] [Accuracy: 90.78]\n",
      "[Deep] [Epoch 14] [Iteration: 399/600] [Loss: 0.272] [Accuracy: 91.99]\n",
      "[Shal] [Epoch 14] [Iteration: 399/600] [Loss: 0.324] [Accuracy: 90.79]\n",
      "[Deep] [Epoch 14] [Iteration: 599/600] [Loss: 0.140] [Accuracy: 91.97]\n",
      "[Shal] [Epoch 14] [Iteration: 599/600] [Loss: 0.220] [Accuracy: 90.89]\n",
      "[Deep] [Epoch 15] [Iteration: 199/600] [Loss: 0.221] [Accuracy: 91.95]\n",
      "[Shal] [Epoch 15] [Iteration: 199/600] [Loss: 0.280] [Accuracy: 90.84]\n",
      "[Deep] [Epoch 15] [Iteration: 399/600] [Loss: 0.272] [Accuracy: 92.09]\n",
      "[Shal] [Epoch 15] [Iteration: 399/600] [Loss: 0.380] [Accuracy: 90.83]\n",
      "[Deep] [Epoch 15] [Iteration: 599/600] [Loss: 0.370] [Accuracy: 92.10]\n",
      "[Shal] [Epoch 15] [Iteration: 599/600] [Loss: 0.424] [Accuracy: 90.92]\n",
      "[Deep] [Epoch 16] [Iteration: 199/600] [Loss: 0.295] [Accuracy: 92.09]\n",
      "[Shal] [Epoch 16] [Iteration: 199/600] [Loss: 0.333] [Accuracy: 90.91]\n",
      "[Deep] [Epoch 16] [Iteration: 399/600] [Loss: 0.218] [Accuracy: 92.08]\n",
      "[Shal] [Epoch 16] [Iteration: 399/600] [Loss: 0.257] [Accuracy: 91.00]\n",
      "[Deep] [Epoch 16] [Iteration: 599/600] [Loss: 0.288] [Accuracy: 92.08]\n",
      "[Shal] [Epoch 16] [Iteration: 599/600] [Loss: 0.302] [Accuracy: 90.92]\n",
      "[Deep] [Epoch 17] [Iteration: 199/600] [Loss: 0.281] [Accuracy: 92.07]\n",
      "[Shal] [Epoch 17] [Iteration: 199/600] [Loss: 0.387] [Accuracy: 90.91]\n",
      "[Deep] [Epoch 17] [Iteration: 399/600] [Loss: 0.343] [Accuracy: 92.16]\n",
      "[Shal] [Epoch 17] [Iteration: 399/600] [Loss: 0.458] [Accuracy: 90.99]\n",
      "[Deep] [Epoch 17] [Iteration: 599/600] [Loss: 0.353] [Accuracy: 92.08]\n",
      "[Shal] [Epoch 17] [Iteration: 599/600] [Loss: 0.421] [Accuracy: 91.02]\n",
      "[Deep] [Epoch 18] [Iteration: 199/600] [Loss: 0.335] [Accuracy: 92.15]\n",
      "[Shal] [Epoch 18] [Iteration: 199/600] [Loss: 0.423] [Accuracy: 91.04]\n",
      "[Deep] [Epoch 18] [Iteration: 399/600] [Loss: 0.209] [Accuracy: 92.17]\n",
      "[Shal] [Epoch 18] [Iteration: 399/600] [Loss: 0.284] [Accuracy: 91.17]\n",
      "[Deep] [Epoch 18] [Iteration: 599/600] [Loss: 0.373] [Accuracy: 92.15]\n",
      "[Shal] [Epoch 18] [Iteration: 599/600] [Loss: 0.337] [Accuracy: 91.10]\n",
      "[Deep] [Epoch 19] [Iteration: 199/600] [Loss: 0.614] [Accuracy: 92.11]\n",
      "[Shal] [Epoch 19] [Iteration: 199/600] [Loss: 0.642] [Accuracy: 91.12]\n",
      "[Deep] [Epoch 19] [Iteration: 399/600] [Loss: 0.119] [Accuracy: 92.10]\n",
      "[Shal] [Epoch 19] [Iteration: 399/600] [Loss: 0.195] [Accuracy: 91.13]\n",
      "[Deep] [Epoch 19] [Iteration: 599/600] [Loss: 0.234] [Accuracy: 92.08]\n",
      "[Shal] [Epoch 19] [Iteration: 599/600] [Loss: 0.261] [Accuracy: 91.18]\n",
      "[Deep] [Epoch 20] [Iteration: 199/600] [Loss: 0.143] [Accuracy: 92.20]\n",
      "[Shal] [Epoch 20] [Iteration: 199/600] [Loss: 0.254] [Accuracy: 91.13]\n",
      "[Deep] [Epoch 20] [Iteration: 399/600] [Loss: 0.330] [Accuracy: 92.14]\n",
      "[Shal] [Epoch 20] [Iteration: 399/600] [Loss: 0.347] [Accuracy: 91.21]\n",
      "[Deep] [Epoch 20] [Iteration: 599/600] [Loss: 0.241] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 20] [Iteration: 599/600] [Loss: 0.295] [Accuracy: 91.22]\n",
      "[Deep] [Epoch 21] [Iteration: 199/600] [Loss: 0.446] [Accuracy: 92.18]\n",
      "[Shal] [Epoch 21] [Iteration: 199/600] [Loss: 0.491] [Accuracy: 91.25]\n",
      "[Deep] [Epoch 21] [Iteration: 399/600] [Loss: 0.331] [Accuracy: 92.24]\n",
      "[Shal] [Epoch 21] [Iteration: 399/600] [Loss: 0.334] [Accuracy: 91.30]\n",
      "[Deep] [Epoch 21] [Iteration: 599/600] [Loss: 0.165] [Accuracy: 92.18]\n",
      "[Shal] [Epoch 21] [Iteration: 599/600] [Loss: 0.230] [Accuracy: 91.18]\n",
      "[Deep] [Epoch 22] [Iteration: 199/600] [Loss: 0.229] [Accuracy: 92.11]\n",
      "[Shal] [Epoch 22] [Iteration: 199/600] [Loss: 0.298] [Accuracy: 91.22]\n",
      "[Deep] [Epoch 22] [Iteration: 399/600] [Loss: 0.232] [Accuracy: 92.23]\n",
      "[Shal] [Epoch 22] [Iteration: 399/600] [Loss: 0.312] [Accuracy: 91.32]\n",
      "[Deep] [Epoch 22] [Iteration: 599/600] [Loss: 0.294] [Accuracy: 92.39]\n",
      "[Shal] [Epoch 22] [Iteration: 599/600] [Loss: 0.370] [Accuracy: 91.27]\n",
      "[Deep] [Epoch 23] [Iteration: 199/600] [Loss: 0.459] [Accuracy: 92.25]\n",
      "[Shal] [Epoch 23] [Iteration: 199/600] [Loss: 0.484] [Accuracy: 91.29]\n",
      "[Deep] [Epoch 23] [Iteration: 399/600] [Loss: 0.254] [Accuracy: 92.24]\n",
      "[Shal] [Epoch 23] [Iteration: 399/600] [Loss: 0.332] [Accuracy: 91.30]\n",
      "[Deep] [Epoch 23] [Iteration: 599/600] [Loss: 0.257] [Accuracy: 92.21]\n",
      "[Shal] [Epoch 23] [Iteration: 599/600] [Loss: 0.288] [Accuracy: 91.43]\n",
      "[Deep] [Epoch 24] [Iteration: 199/600] [Loss: 0.327] [Accuracy: 92.24]\n",
      "[Shal] [Epoch 24] [Iteration: 199/600] [Loss: 0.340] [Accuracy: 91.40]\n",
      "[Deep] [Epoch 24] [Iteration: 399/600] [Loss: 0.193] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 24] [Iteration: 399/600] [Loss: 0.269] [Accuracy: 91.33]\n",
      "[Deep] [Epoch 24] [Iteration: 599/600] [Loss: 0.278] [Accuracy: 92.26]\n",
      "[Shal] [Epoch 24] [Iteration: 599/600] [Loss: 0.351] [Accuracy: 91.44]\n",
      "[Deep] [Epoch 25] [Iteration: 199/600] [Loss: 0.307] [Accuracy: 92.24]\n",
      "[Shal] [Epoch 25] [Iteration: 199/600] [Loss: 0.338] [Accuracy: 91.39]\n",
      "[Deep] [Epoch 25] [Iteration: 399/600] [Loss: 0.226] [Accuracy: 92.38]\n",
      "[Shal] [Epoch 25] [Iteration: 399/600] [Loss: 0.284] [Accuracy: 91.48]\n",
      "[Deep] [Epoch 25] [Iteration: 599/600] [Loss: 0.271] [Accuracy: 92.30]\n",
      "[Shal] [Epoch 25] [Iteration: 599/600] [Loss: 0.296] [Accuracy: 91.47]\n",
      "[Deep] [Epoch 26] [Iteration: 199/600] [Loss: 0.273] [Accuracy: 92.30]\n",
      "[Shal] [Epoch 26] [Iteration: 199/600] [Loss: 0.319] [Accuracy: 91.49]\n",
      "[Deep] [Epoch 26] [Iteration: 399/600] [Loss: 0.200] [Accuracy: 92.31]\n",
      "[Shal] [Epoch 26] [Iteration: 399/600] [Loss: 0.255] [Accuracy: 91.49]\n",
      "[Deep] [Epoch 26] [Iteration: 599/600] [Loss: 0.293] [Accuracy: 92.26]\n",
      "[Shal] [Epoch 26] [Iteration: 599/600] [Loss: 0.334] [Accuracy: 91.45]\n",
      "[Deep] [Epoch 27] [Iteration: 199/600] [Loss: 0.353] [Accuracy: 92.13]\n",
      "[Shal] [Epoch 27] [Iteration: 199/600] [Loss: 0.392] [Accuracy: 91.51]\n",
      "[Deep] [Epoch 27] [Iteration: 399/600] [Loss: 0.349] [Accuracy: 92.17]\n",
      "[Shal] [Epoch 27] [Iteration: 399/600] [Loss: 0.334] [Accuracy: 91.48]\n",
      "[Deep] [Epoch 27] [Iteration: 599/600] [Loss: 0.265] [Accuracy: 92.38]\n",
      "[Shal] [Epoch 27] [Iteration: 599/600] [Loss: 0.328] [Accuracy: 91.53]\n",
      "[Deep] [Epoch 28] [Iteration: 199/600] [Loss: 0.173] [Accuracy: 92.45]\n",
      "[Shal] [Epoch 28] [Iteration: 199/600] [Loss: 0.193] [Accuracy: 91.48]\n",
      "[Deep] [Epoch 28] [Iteration: 399/600] [Loss: 0.221] [Accuracy: 92.26]\n",
      "[Shal] [Epoch 28] [Iteration: 399/600] [Loss: 0.312] [Accuracy: 91.50]\n",
      "[Deep] [Epoch 28] [Iteration: 599/600] [Loss: 0.286] [Accuracy: 92.33]\n",
      "[Shal] [Epoch 28] [Iteration: 599/600] [Loss: 0.299] [Accuracy: 91.58]\n",
      "[Deep] [Epoch 29] [Iteration: 199/600] [Loss: 0.163] [Accuracy: 92.28]\n",
      "[Shal] [Epoch 29] [Iteration: 199/600] [Loss: 0.237] [Accuracy: 91.60]\n",
      "[Deep] [Epoch 29] [Iteration: 399/600] [Loss: 0.158] [Accuracy: 92.26]\n",
      "[Shal] [Epoch 29] [Iteration: 399/600] [Loss: 0.239] [Accuracy: 91.56]\n",
      "[Deep] [Epoch 29] [Iteration: 599/600] [Loss: 0.237] [Accuracy: 92.33]\n",
      "[Shal] [Epoch 29] [Iteration: 599/600] [Loss: 0.297] [Accuracy: 91.58]\n"
     ]
    }
   ],
   "source": [
    "# Step 7. Train Model\n",
    "# 임의의 학습 이미지를 가져옵니다\n",
    "dataiter = iter(train_loader)\n",
    "images, _ = dataiter.next()\n",
    "writer_deep.add_graph(model_deep, images.view(-1, 28 * 28)) #tensorboard에 기록\n",
    "writer_shallow.add_graph(model_shallow, images.view(-1, 28 * 28))\n",
    "\n",
    "loss_deep = 0\n",
    "loss_shallow = 0\n",
    "total_iter = 0\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        labels = labels\n",
    "        images = images.to(devices_id)#gpu 혹은 cpu에 보내기\n",
    "        labels = labels.to(devices_id)\n",
    "\n",
    "        optimizer_deep.zero_grad()\n",
    "        optimizer_shallow.zero_grad()\n",
    "        outputs_deep = model_deep(images)\n",
    "        outputs_shallow = model_shallow(images)\n",
    "        # Calc loss\n",
    "        loss_deep = criterion(outputs_deep, labels)\n",
    "        loss_shallow = criterion(outputs_shallow, labels)\n",
    "        # Back-propagation\n",
    "        loss_deep.backward()\n",
    "        loss_shallow.backward()\n",
    "        # Updating wegihts\n",
    "        optimizer_deep.step()\n",
    "        optimizer_shallow.step()\n",
    "\n",
    "        total_iter += 1\n",
    "        if total_iter < int(600*epochs - 10):\n",
    "            writer_deep.add_scalar('Train/Loss', loss_deep, total_iter)\n",
    "            writer_shallow.add_scalar('Train/Loss', loss_shallow, total_iter)\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            # calculate Accuracy\n",
    "            correct_deep = 0\n",
    "            correct_shallow = 0\n",
    "            total = 0\n",
    "\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28 * 28)\n",
    "                images = images.to(devices_id)\n",
    "                \n",
    "                outputs_deep = model_deep(images)\n",
    "                outputs_shallow = model_shallow(images)\n",
    "\n",
    "                _, predicted_deep = torch.max(outputs_deep.data, 1)\n",
    "                _, predicted_shallow = torch.max(outputs_shallow.data, 1)\n",
    "                total += labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                predicted_deep = predicted_deep.cpu()\n",
    "                predicted_shallow = predicted_shallow.cpu()\n",
    "\n",
    "                correct_deep += (predicted_deep == labels).sum()\n",
    "                correct_shallow += (predicted_shallow == labels).sum()\n",
    "\n",
    "            accuracy_deep = 100 * correct_deep / total\n",
    "            accuracy_shallow = 100 * correct_shallow / total\n",
    "\n",
    "            print(\n",
    "                f\"[Deep] [Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss_deep.item():.3f}] [Accuracy: {accuracy_deep:.2f}]\"\n",
    "            )\n",
    "            print(\n",
    "                f\"[Shal] [Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss_shallow.item():.3f}] [Accuracy: {accuracy_shallow:.2f}]\"\n",
    "            )\n",
    "\n",
    "writer_deep.close()\n",
    "writer_shallow.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}